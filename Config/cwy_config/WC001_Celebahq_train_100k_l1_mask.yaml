---
# pytorch_lightning==1.9.1
# WC001_Celebahq_train_100k_l1_mask.yaml
seed_everything: true
trainer:
  # note: specify your loggers here as a list
  logger:
    - class_path: pytorch_lightning.loggers.TensorBoardLogger # note: enable logging to tensorboard, support image logging
      init_args:
        save_dir: "Experiment"            # note: directory to save log files
        name: "WC001_Celebahq_train_100k_l1_mask"          # note: name of the current experiment
        version: "tensorboard_train_val"  # note: version name specified by the logger, for formally experiments, please specify a version number
    - class_path: pytorch_lightning.loggers.CSVLogger # note: enable logging to csv, doesn't support image logging
      init_args:
        save_dir: "Experiment"            # note: identical to other loggers' is ok, they'll save log files to the same directory
        name: "WC001_Celebahq_train_100k_l1_mask"          # note: keep the same experiment name
        version: "csv_train_val"          # note: version name specified by the logger, for formally experiments, please specify a version number
  enable_checkpointing: true              # note: lightning save the last.ckpt for you if set to true, can be overwritten by ModelCheckpoint, see following callbacks
  callbacks:
    - class_path: pytorch_lightning.callbacks.progress.TQDMProgressBar # note: display a tqdm progress bar
      init_args:
        refresh_rate: 50 # note: after processing these batches, tqdm'll update its display
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint # note: specify ModelCheckpoint behavior
      init_args:
        save_last: true # note: save the last.ckpt which contains the latest net weights and optimizer parameters
        every_n_epochs: 100 # note: save .ckpt every n epochs
        filename: "snpatchgan_{epoch:02d}" # note: inject epoch into .ckpt filename, make sure you had
        save_top_k: -1
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        monitor: "val_metric_l1_err" # note: save .ckpt with the smallest l1_err
        filename: "snpatchgan_best_l1_{epoch:02d}_{val_metric_l1_err:.4f}_{val_metric_l2_err:.4f}" # note: inject val_metric_l1_err & val_metric_l2_err into .ckpt filename, make sure you had logged these two metrics in validation_step()
    - class_path: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
      init_args:
        monitor: "val_metric_l2_err" # note: save .ckpt with the smallest l2_err
        filename: "snpatchgan_best_l2_{epoch:02d}_{val_metric_l1_err:.4f}_{val_metric_l2_err:.4f}"  # note: inject val_metric_l1_err & val_metric_l2_err into .ckpt filename, make sure you had logged these two metrics in validation_step()
  default_root_dir: "Experiment/WC001_Celebahq_train_100k_l1_mask" # note: specify the trainer's root directory, namely the default location to save artifacts of loggers, checkpoints etc.
  gradient_clip_val: null
  gradient_clip_algorithm: null
  num_nodes: 1
  num_processes: null
  devices: [1] # note: there are two ways: 1. specify your GPU number(s) for acceleration as a list;  2. set an integer that indicates how many GPUs to use, lightning will do auto-detection for you
  gpus: null
  auto_select_gpus: null
  tpu_cores: null
  ipus: null
  enable_progress_bar: true # note: whether to enable to progress bar by default, if true, tqdm'll be displayed with default settings, can be overwritten by callback TQDMProgressBar, see above
  overfit_batches: 0.0
  track_grad_norm: -1
  check_val_every_n_epoch: null
  fast_dev_run: false
  accumulate_grad_batches: null
  max_epochs: null # note: stop training once this number of epochs is reached
  min_epochs: null
  max_steps: 100000 # note: stop training after this number of steps(i.e. iterations)
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: 1.0 # note: how much of validation dataset to check, float number as ratio to check, interger as the exact batches to check, set to 0 to disable validating
  limit_test_batches: null
  limit_predict_batches: null
  val_check_interval: 1000 # note: how often to check the validation set.
  # Pass a float in the range [0.0, 1.0] to check after a fraction of the training epoch.
  # Pass an int to check after a fixed number of training batches.
  # An int value can only be higher than the number of training batches when check_val_every_n_epoch=None,
  # which validates after every N training batches across epochs or during iteration-based training
  log_every_n_steps: 100 # note: how often to log within steps(i.e. iterations)
  accelerator: "gpu" # note: may be 'cpu' 'gpu' 'tpu' 'ipu' 'hpu' 'mps', use 'auto' for auto-detection
  strategy: "ddp" # note: use distributed data parallel strategy
  sync_batchnorm: false
  precision: 32
  enable_model_summary: true
  num_sanity_val_steps: 2
  resume_from_checkpoint: null
  profiler: null
  benchmark: null
  deterministic: null
  reload_dataloaders_every_n_epochs: 0
  auto_lr_find: false
  replace_sampler_ddp: true
  detect_anomaly: false
  auto_scale_batch_size: false
  plugins: null
  amp_backend: null
  amp_level: null
  move_metrics_to_cpu: false
  multiple_trainloader_mode: max_size_cycle
  inference_mode: true
model: # note:: specify your model(LightningModule) here
  class_path: Code.Model.SNPatchGAN
  init_args:
    image_height: 256 # note: must be identical to out_shape defined by data, see below
    image_width: 256 # note: must be identical to out_shape defined by data, see below
    image_channel: 3
    mask_height: 128
    mask_width: 128
    max_delta_height: 32 # note: the maximum value for mask height shrink, the shrink value is randomly sampled within [0, max_delta_height]
    max_delta_width: 32 # note: the maximum value for mask width shrink, the shrink value is randomly sampled within [0, max_delta_width]
    vertical_margin: 0 # note: mask won't reach vertical margin pixels within vertical_margin range to the image boundary
    horizontal_margin: 0 # note: mask won't reach vertical margin pixels within horizontal_margin range to the image boundary
    guided: false # note: whether to use edge map for guidance
    batch_size: 16 # note: this is not a real batch_size for training, just for (input shape, output shape) sanity check
    l1_loss: true # note: whether to include l1_loss as part of generator total loss function
    l1_loss_alpha: 1.0 # note: the balancer for l1_loss
    gan_loss_alpha: 1.0 # note: the balancer for gan hinge_loss
    gan_with_mask: true # note: whether input mask to discriminator
    # note: parameters for Adam optimizers (generator optimizer & discriminator optimizer)
    lr: 0.0001
    b1: 0.5
    b2: 0.999
    save_dir: "Experiment/WC001_Celebahq_train_100k_l1_mask/saved_images" # note: directory to save images generated by generator for visually check
    prefix: "gen_" # note: add a file name prefix to the saved images
data: # note: specify your data manager(LightningDataModule) here
  class_path: DataModule.CelebAMaskHQ
  init_args:
    train_data_dir: "/mnt/data1/Datasets/CelebAMask-HQ/CelebA-HQ-img"
    out_shape: [256, 256] # note: output image shape will be croped (and may be resized if image is too small) to out_shape
    batch_size: 16 # note: batch_size is nominated by this value
    num_workers: 4
    validation_ratio: 0.1
    test_ratio: 0.1
ckpt_path: null
...